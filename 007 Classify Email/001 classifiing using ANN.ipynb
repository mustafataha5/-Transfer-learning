{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d3fe0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded data with encoding: utf-8\n",
      "\n",
      "DataFrame Head:\n",
      "   Spam                                            Message Unnamed: 2  \\\n",
      "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
      "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
      "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
      "\n",
      "  Unnamed: 3 Unnamed: 4  \n",
      "0        NaN        NaN  \n",
      "1        NaN        NaN  \n",
      "2        NaN        NaN  \n",
      "3        NaN        NaN  \n",
      "4        NaN        NaN  \n",
      "\n",
      "DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 5 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Spam        5572 non-null   object\n",
      " 1   Message     5572 non-null   object\n",
      " 2   Unnamed: 2  50 non-null     object\n",
      " 3   Unnamed: 3  12 non-null     object\n",
      " 4   Unnamed: 4  6 non-null      object\n",
      "dtypes: object(5)\n",
      "memory usage: 217.8+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "def load_data(file_path):\n",
    "    \"\"\"\n",
    "    Load data from a CSV file into a pandas DataFrame, trying common encodings.\n",
    "    \n",
    "    Parameters:\n",
    "    file_path (str): The path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the loaded data, or None if loading fails.\n",
    "    \"\"\"\n",
    "    encodings_to_try = ['utf-8', 'latin1', 'ISO-8859-1', 'cp1252']\n",
    "    \n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            data = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"Successfully loaded data with encoding: {encoding}\")\n",
    "            return data\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to load with encoding: {encoding}\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            return None\n",
    "            \n",
    "    print(\"Could not load the file with any of the attempted encodings.\")\n",
    "    return None\n",
    "\n",
    "# Attempt to load the data with the updated function\n",
    "df = load_data('spam.csv')\n",
    "\n",
    "# If successful, you can then display the DataFrame\n",
    "if df is not None:\n",
    "    print(\"\\nDataFrame Head:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataFrame Info:\")\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e81a6b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                            Message\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will �_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[[\"Spam\", \"Message\"]]  # Adjusting the DataFrame to keep only relevant columns\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52d7b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import string \n",
    "import re\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))  # Get English stopwords\n",
    "porterStemmer = PorterStemmer()  # Initialize the stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099f652b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n",
    "    tokens = [word for word in tokens if word.isalpha()]  # Remove punctuation and non-alphabetic characters\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]  # Remove punctuation\n",
    "    \n",
    "    tokens = [token for token in tokens if token not in stopwords]  # Remove\n",
    "\n",
    "    \n",
    "    tokens = [porterStemmer.stem(token) for token in tokens]  # Stem the tokens\n",
    "     \n",
    "    processed_text = ' '.join(tokens)  # Join tokens back into a single string\n",
    "\n",
    "    processed_text = re.sub(r'http\\S+|www\\S+|https\\S+', '', processed_text, flags=re.MULTILINE)  # Remove URLs\n",
    "    processed_text = re.sub(r'\\@\\w+|\\#', '', processed_text)  # Remove mentions and hashtags\n",
    "    processed_text = re.sub(r'\\d+', '', processed_text)  # Remove digits\n",
    "    processed_text = re.sub(r'\\s+', ' ', processed_text).strip()  # Remove extra whitespace\n",
    "    return processed_text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e956bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Processed_Message'] = df['Message'].apply(preprocess_text)  # Apply preprocessing to the 'Message' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d30ff390",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Spam</th>\n",
       "      <th>Message</th>\n",
       "      <th>Processed_Message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>go jurong point crazi avail bugi n great world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ok lar joke wif u oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>free entri wkli comp win fa cup final tkt may ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>u dun say earli hor u c alreadi say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>nah think goe usf live around though</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "      <td>time tri contact u pound prize claim easi call...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will �_ b going to esplanade fr home?</td>\n",
       "      <td>b go esplanad fr home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "      <td>piti mood suggest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "      <td>guy bitch act like interest buy someth els nex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "      <td>rofl true name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Spam                                            Message  \\\n",
       "0      ham  Go until jurong point, crazy.. Available only ...   \n",
       "1      ham                      Ok lar... Joking wif u oni...   \n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "3      ham  U dun say so early hor... U c already then say...   \n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...   \n",
       "...    ...                                                ...   \n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...   \n",
       "5568   ham              Will �_ b going to esplanade fr home?   \n",
       "5569   ham  Pity, * was in mood for that. So...any other s...   \n",
       "5570   ham  The guy did some bitching but I acted like i'd...   \n",
       "5571   ham                         Rofl. Its true to its name   \n",
       "\n",
       "                                      Processed_Message  \n",
       "0     go jurong point crazi avail bugi n great world...  \n",
       "1                                 ok lar joke wif u oni  \n",
       "2     free entri wkli comp win fa cup final tkt may ...  \n",
       "3                   u dun say earli hor u c alreadi say  \n",
       "4                  nah think goe usf live around though  \n",
       "...                                                 ...  \n",
       "5567  time tri contact u pound prize claim easi call...  \n",
       "5568                              b go esplanad fr home  \n",
       "5569                                  piti mood suggest  \n",
       "5570  guy bitch act like interest buy someth els nex...  \n",
       "5571                                     rofl true name  \n",
       "\n",
       "[5572 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2185bb51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Spam\n",
       "ham     4825\n",
       "spam     747\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Spam'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d6df7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resampled dataset shape: Counter({0: 4825, 1: 4825})\n"
     ]
    }
   ],
   "source": [
    "X = df['Processed_Message']  # Features\n",
    "y = df['Spam'].map({'ham':0,'spam':1 })  # Target variable\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# 1. Initialize TfidfVectorizer\n",
    "# You can adjust parameters like max_features, min_df, max_df as needed\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000) # Example: consider top 5000 words\n",
    "\n",
    "# 2. Transform the text data into numerical TF-IDF features\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(X)\n",
    "\n",
    "# Now X_tfidf is a sparse matrix of numerical features.\n",
    "# SMOTE can work with sparse matrices.\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# 3. Apply SMOTE to the numerical (TF-IDF) data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X_tfidf, y)\n",
    "\n",
    "print(f\"Resampled dataset shape: {Counter(y_resampled)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82cdcd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U scikit-learn imbalanced-learn\n",
    "# !pip uninstall -y imbalance-learn\n",
    "# !pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3a972a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# import imblearn\n",
    "\n",
    "# print(\"scikit-learn version:\", sklearn.__version__)\n",
    "# print(\"imblearn version:\", imblearn.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d307978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# 4. Split the resampled data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "88e4945e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\ai_env\\lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(Dropout(0.5))  # Add dropout for regularization\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))  # Assuming 32 classes for the output layer\n",
    "model.add(Dense(1, activation='sigmoid'))  # For binary classification (spam or not spam)\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6b9dc000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 9ms/step - accuracy: 0.7629 - loss: 0.4191 - val_accuracy: 0.9850 - val_loss: 0.0412\n",
      "Epoch 2/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9881 - loss: 0.0286 - val_accuracy: 0.9902 - val_loss: 0.0246\n",
      "Epoch 3/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9954 - loss: 0.0125 - val_accuracy: 0.9912 - val_loss: 0.0261\n",
      "Epoch 4/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9965 - loss: 0.0074 - val_accuracy: 0.9896 - val_loss: 0.0298\n",
      "Epoch 5/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9985 - loss: 0.0049 - val_accuracy: 0.9922 - val_loss: 0.0257\n",
      "Epoch 6/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9980 - loss: 0.0043 - val_accuracy: 0.9834 - val_loss: 0.0569\n",
      "Epoch 7/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9973 - loss: 0.0059 - val_accuracy: 0.9912 - val_loss: 0.0304\n",
      "Epoch 8/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9977 - loss: 0.0047 - val_accuracy: 0.9865 - val_loss: 0.0471\n",
      "Epoch 9/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9980 - loss: 0.0040 - val_accuracy: 0.9865 - val_loss: 0.0494\n",
      "Epoch 10/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9981 - loss: 0.0045 - val_accuracy: 0.9876 - val_loss: 0.0477\n",
      "Epoch 11/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - accuracy: 0.9977 - loss: 0.0054 - val_accuracy: 0.9865 - val_loss: 0.0468\n",
      "Epoch 12/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9976 - loss: 0.0047 - val_accuracy: 0.9881 - val_loss: 0.0417\n",
      "Epoch 13/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9985 - loss: 0.0034 - val_accuracy: 0.9907 - val_loss: 0.0367\n",
      "Epoch 14/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9974 - loss: 0.0054 - val_accuracy: 0.9886 - val_loss: 0.0431\n",
      "Epoch 15/15\n",
      "\u001b[1m242/242\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9982 - loss: 0.0040 - val_accuracy: 0.9902 - val_loss: 0.0428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23377cda590>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=15, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c52a00e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step\n",
      "The message is classified as spam.\n"
     ]
    }
   ],
   "source": [
    "massage = \"Congratulations! You've won a free ticket to Bahamas!\"\n",
    "\n",
    "# Preprocess the new message\n",
    "new_message_processed = preprocess_text(massage)\n",
    "new_message_tfidf = tfidf_vectorizer.transform([new_message_processed])\n",
    "# Predict using the trained model\n",
    "prediction = model.predict(new_message_tfidf)\n",
    "if prediction[0][0] > 0.5:\n",
    "    print(\"The message is classified as spam.\")\n",
    "else:\n",
    "    print(\"The message is classified as ham (not spam).\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a26b9567",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9908 - loss: 0.0413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.04281359910964966, 0.9901554584503174]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c6f34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
