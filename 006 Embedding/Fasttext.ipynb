{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6ef396bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Arabic text preprocessing complete. Output saved to 'processed_data.txt'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def remove_tashkeel(text):\n",
    "    \"\"\"\n",
    "    Removes Arabic diacritics (tashkeel) and other non-letter marks from the text.\n",
    "    This function targets a comprehensive set of Unicode characters associated with\n",
    "    Arabic vocalization and annotation marks, including:\n",
    "    - Standard tashkeel (Fatha, Damma, Kasra, Sukun, Shadda, Tanween).\n",
    "    - Quranic annotation signs and small Kufic marks (e.g., small Fatha, Damma, Kasra).\n",
    "    - Superscript Alef (الألف الخنجرية).\n",
    "    - Other less common vowel signs and diacritics.\n",
    "\n",
    "    Unicode ranges covered:\n",
    "    - U+0610 to U+061A (Arabic Small Kufic Marks, Quranic annotation signs)\n",
    "    - U+064B to U+065F (Standard Tashkeel and additional diacritics/vowel signs)\n",
    "    - U+0670 (Arabic Letter Superscript Alef)\n",
    "    - U+06D6 to U+06DC (Quranic Annoation Signs)\n",
    "    - U+06DF to U+06E8 (Quranic Annoation Signs)\n",
    "    - U+06EA to U+06ED (Quranic Annoation Signs)\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\u0610-\\u061A\\u064B-\\u065F\\u0670\\u06D6-\\u06DC\\u06DF-\\u06E8\\u06EA-\\u06ED]', '', text)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def normalize_aleft(text):\n",
    "    \"\"\"\n",
    "    Normalizes different forms of the Arabic letter Alef (أ, إ, آ, ٱ) to a standard Alef (ا).\n",
    "    This helps in standardizing text for consistent processing.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[أإآٱ]', 'ا', text)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def remove_tatweel(text):\n",
    "    \"\"\"\n",
    "    Removes the Tatweel (elongation) character (ـ) from the text.\n",
    "    Tatweel is used to stretch words visually but carries no linguistic meaning in most NLP tasks.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[\\u0640]', '', text)\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def remove_punctuation_and_non_arabic(text):\n",
    "    \"\"\"\n",
    "    Removes punctuation and any non-Arabic characters, keeping only Arabic letters and spaces.\n",
    "    This is a crucial step to clean text from irrelevant symbols.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n",
    "\n",
    "def preprocess_arabic_text(text):\n",
    "    \"\"\"\n",
    "    Applies a series of preprocessing steps to Arabic text in a specific order:\n",
    "    1.  `remove_tashkeel`: Eliminates all diacritics and Quranic annotation marks.\n",
    "    2.  `normalize_aleft`: Converts various Alef forms to a single standard form.\n",
    "    3.  `remove_tatweel`: Removes the Tatweel character.\n",
    "    4.  `remove_punctuation_and_non_arabic`: Filters out any remaining punctuation or non-Arabic symbols.\n",
    "    5.  `re.sub(r'\\s+', ' ', text).strip()`: Normalizes multiple spaces into a single space\n",
    "        and removes leading/trailing whitespace, ensuring clean word separation.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input Arabic text string to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        str: The preprocessed and cleaned Arabic text string.\n",
    "    \"\"\"\n",
    "    text = remove_tashkeel(text)\n",
    "    text = normalize_aleft(text)\n",
    "    text = remove_tatweel(text)\n",
    "    text = remove_punctuation_and_non_arabic(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip() # Normalize spaces and strip leading/trailing whitespace\n",
    "    return text\n",
    "\n",
    "# --- Example Usage and File Operations ---\n",
    "\n",
    "# Define file paths for input and output data.\n",
    "# Ensure 'data.txt' exists in the same directory as this script,\n",
    "# or provide its full path.\n",
    "new_text_data_path = 'data.txt'\n",
    "processed_file_path = 'processed_data.txt'\n",
    "\n",
    "# Read, preprocess, and write text from the specified file.\n",
    "try:\n",
    "    # Open the input file for reading with UTF-8 encoding.\n",
    "    with open(new_text_data_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines() # Read all lines into a list\n",
    "\n",
    "    # Process each line using the preprocess_arabic_text function.\n",
    "    # The lru_cache on sub-functions will optimize repeated processing of identical substrings.\n",
    "    processed_lines = [preprocess_arabic_text(line) for line in lines]\n",
    "\n",
    "    # Open the output file for writing with UTF-8 encoding.\n",
    "    # Each processed line is written followed by a newline character.\n",
    "    with open(processed_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.writelines(line + '\\n' for line in processed_lines)\n",
    "\n",
    "    print(\"✅ Arabic text preprocessing complete. Output saved to 'processed_data.txt'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    # Handle the case where the input file does not exist.\n",
    "    print(f\"Error: The input file '{new_text_data_path}' was not found. Please ensure it exists.\")\n",
    "except Exception as e:\n",
    "    # Catch any other potential errors during file operations or processing.\n",
    "    print(f\"An unexpected error occurred during file processing: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66dcec59-eb35-4311-8353-1ff7f1e45fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetuning fasttext model on a custom dataset\n",
    "import fasttext\n",
    "\n",
    "new_model = fasttext.train_supervised( input=processed_file_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e35f8ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'fasttext_model.bin'\n",
    "new_model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4297165b",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = fasttext.load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f3ae82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>',\n",
       " 'الرحمن',\n",
       " 'الرحيم',\n",
       " 'عليهم',\n",
       " 'بسم',\n",
       " 'الله',\n",
       " 'الحمد',\n",
       " 'لله',\n",
       " 'رب',\n",
       " 'العلمين',\n",
       " 'ملك',\n",
       " 'يوم',\n",
       " 'الدين',\n",
       " 'اياك',\n",
       " 'نعبد',\n",
       " 'واياك',\n",
       " 'نستعين',\n",
       " 'اهدناالصرط',\n",
       " 'المستقيم',\n",
       " 'صرط',\n",
       " 'الذين',\n",
       " 'انعمت',\n",
       " 'غير',\n",
       " 'المغضوب',\n",
       " 'ولا',\n",
       " 'الضالين']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_model.get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7cd48875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'الرحمن': [ 0.00238093  0.00509113  0.00831976 -0.00927015  0.00661152  0.00807286\n",
      " -0.00648016  0.00133566 -0.00343636 -0.00210862 -0.00070896 -0.00633463\n",
      " -0.00485001  0.00221098 -0.00606992 -0.00717415  0.00338688  0.00603667\n",
      " -0.00688208 -0.00057144 -0.00488543  0.00987955  0.00397523  0.00694702\n",
      " -0.00861237 -0.00616428 -0.00983707 -0.00120694 -0.00220774 -0.00728686\n",
      " -0.00501538 -0.00077216  0.00396778 -0.00586287 -0.00635476 -0.00211214\n",
      " -0.00715075  0.00431303 -0.00053761 -0.0027656  -0.0099938   0.00944217\n",
      " -0.00514806  0.00308559  0.00880401  0.00192514  0.00807715 -0.00336937\n",
      " -0.00391913  0.00350796 -0.00100431 -0.00181935  0.0023661   0.00489011\n",
      " -0.00905311  0.00352073 -0.00963604  0.0006935   0.00876348 -0.00243911\n",
      " -0.00015379  0.00351502  0.00568376  0.00787363  0.00454237 -0.00356971\n",
      " -0.00709526 -0.00525634 -0.00576154 -0.00568993  0.00156353  0.00505632\n",
      "  0.0058047  -0.00733124 -0.0023909  -0.00252901 -0.00390956  0.00289598\n",
      "  0.00645355  0.00565575  0.00048762 -0.0058168   0.00268054  0.00279135\n",
      "  0.00477892  0.00360709 -0.00201188 -0.00985829 -0.00474296  0.00727802\n",
      "  0.00288402  0.00248757  0.00252069  0.00179133  0.00933011  0.00230927\n",
      " -0.0006344   0.00242033 -0.00682412 -0.00715008]\n"
     ]
    }
   ],
   "source": [
    "word = 'الرحمن'\n",
    "word_vector = load_model.get_word_vector(word)\n",
    "print(f\"Vector for '{word}': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7872995f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "C:/Users/user/Desktop/hasob_win/Transfer learning/MyCode/005 Text processing/cc.ar.300.vec cannot be opened for loading!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fine_tune_model \u001b[38;5;241m=\u001b[39m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_supervised\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprocessed_data.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrainedVectors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/user/Desktop/hasob_win/Transfer learning/MyCode/005 Text processing/cc.ar.300.vec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfasttext_model.bin\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m fine_tune_model\u001b[38;5;241m.\u001b[39msave_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\ai_env\\lib\\site-packages\\fasttext\\FastText.py:528\u001b[0m, in \u001b[0;36mtrain_supervised\u001b[1;34m(*kargs, **kwargs)\u001b[0m\n\u001b[0;32m    526\u001b[0m a \u001b[38;5;241m=\u001b[39m _build_args(args, manually_set_args)\n\u001b[0;32m    527\u001b[0m ft \u001b[38;5;241m=\u001b[39m _FastText(args\u001b[38;5;241m=\u001b[39ma)\n\u001b[1;32m--> 528\u001b[0m \u001b[43mfasttext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    529\u001b[0m ft\u001b[38;5;241m.\u001b[39mset_args(ft\u001b[38;5;241m.\u001b[39mf\u001b[38;5;241m.\u001b[39mgetArgs())\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ft\n",
      "\u001b[1;31mValueError\u001b[0m: C:/Users/user/Desktop/hasob_win/Transfer learning/MyCode/005 Text processing/cc.ar.300.vec cannot be opened for loading!"
     ]
    }
   ],
   "source": [
    "fine_tune_model = fasttext.train_supervised(\n",
    "    input='processed_data.txt',\n",
    "    dim=300,\n",
    "    pretrainedVectors='C:/Users/user/Desktop/hasob_win/Transfer learning/MyCode/005 Text processing/cc.ar.300.vec'\n",
    ")\n",
    "model_path = 'fasttext_model.bin'\n",
    "fine_tune_model.save_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fa0abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7510d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'الرحمن': [ 0.00238093  0.00509113  0.00831976 -0.00927015  0.00661152  0.00807286\n",
      " -0.00648016  0.00133566 -0.00343636 -0.00210862 -0.00070896 -0.00633463\n",
      " -0.00485001  0.00221098 -0.00606992 -0.00717415  0.00338688  0.00603667\n",
      " -0.00688208 -0.00057144 -0.00488543  0.00987955  0.00397523  0.00694702\n",
      " -0.00861237 -0.00616428 -0.00983707 -0.00120694 -0.00220774 -0.00728686\n",
      " -0.00501538 -0.00077216  0.00396778 -0.00586287 -0.00635476 -0.00211214\n",
      " -0.00715075  0.00431303 -0.00053761 -0.0027656  -0.0099938   0.00944217\n",
      " -0.00514806  0.00308559  0.00880401  0.00192514  0.00807715 -0.00336937\n",
      " -0.00391913  0.00350796 -0.00100431 -0.00181935  0.0023661   0.00489011\n",
      " -0.00905311  0.00352073 -0.00963604  0.0006935   0.00876348 -0.00243911\n",
      " -0.00015379  0.00351502  0.00568376  0.00787363  0.00454237 -0.00356971\n",
      " -0.00709526 -0.00525634 -0.00576154 -0.00568993  0.00156353  0.00505632\n",
      "  0.0058047  -0.00733124 -0.0023909  -0.00252901 -0.00390956  0.00289598\n",
      "  0.00645355  0.00565575  0.00048762 -0.0058168   0.00268054  0.00279135\n",
      "  0.00477892  0.00360709 -0.00201188 -0.00985829 -0.00474296  0.00727802\n",
      "  0.00288402  0.00248757  0.00252069  0.00179133  0.00933011  0.00230927\n",
      " -0.0006344   0.00242033 -0.00682412 -0.00715008]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word = 'الرحمن'\n",
    "word_vector = load_model.get_word_vector(word)\n",
    "print(f\"Vector for '{word}': {word_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbf1b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
